{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO8010: Homework 1\n",
    "In this first homework you will familiarize yourself with the basics of differentiable programming with PyTorch. At the end of this assignment you should be able to build and train your own multi-layer perceptron! Although this assignment is optional, we **strongly** advise you to do it. At the end of the course we expect that you will be comfortable with PyTorch, which you will in turn also use for your final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the ***wonderful world of Deep Learning*** you need to install some libraries. \n",
    "If you are reading these instructions, it means that you already have the Jupyter-Notebook running, which is a good start since you will have to use it for both assignments. \n",
    "\n",
    "In order to successfully run all the cells, you will need some libraries installed on your machine. Together with this assignment you are also given a `.pdf` and a `.yml` file which will guide you through the installation process. Please read the instructions carefully before starting this homework.\n",
    "\n",
    "If you happen to not be familiar with Jupyter we refer you to this straightforward [tutorial](https://realpython.com/jupyter-notebook-introduction/) which should help you in getting started.\n",
    "\n",
    "You will notice that the assignemnt comes in different flavours: sometimes you will be provided with already running code, which comes with appropriate explanations. Other times you will have to write code yourself, when this is the case the cells of the notebook will be marked in red with the following instruction <span style=\"color:red; font-style: italic\">Your code comes below</span>. Sometimes, next to writing code yourself you will also have to motivate why you programmed certain things or guide us quickly through the results that you obtained. If this is required you will see the following green instruction: <span style=\"color:green; font-style:italic\">Your discussion comes below</span>. \n",
    "\n",
    "Please note that you will **not** have to handle in any sort of written report by the end of the assignment. We do however expect **the notebook** with the solutions to the exercises.\n",
    "\n",
    "Now that you have survived the boring part you are ready to start this first assignment!\n",
    "Our first step is very easy, we just start with importing some appropriate libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors and basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are one of the main ingredients when it comes to modern Deep-Learning frameworks. Almost all deep learning computations can be expressed as tensor operations which make computation fast and efficient. We will now see how to manipulate tensors within the PyTorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 A guided tour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a tensor?** There are many valid answers but to keep it simple you can see them as multidimensional arrays. \n",
    "\n",
    "Wikipedia says: *'A tensor may be represented as a (potentially multidimensional) array. Just as a vector in an n-dimensional space is represented by a one-dimensional array with n components with respect to a given basis, any tensor with respect to a basis is represented by a multidimensional array.'*\n",
    "\n",
    "*In PyTorch, a tensor is an object from the class torch.Tensor (or torch.Tensor).*\n",
    "This class handles almost any operations you could like to perform on a tensor, see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to create tensors, here below are two examples (for further information you can check the [library](https://pytorch.org/docs/stable/tensors.html#:~:text=To%20create%20a%20tensor%20with,ops%20(see%20Creation%20Ops) \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a tensor from an array:\n",
    "t1 = torch.tensor([[1, 2], [7, 9]])\n",
    "# Or we can also create a 2x2 tensor filled with zeros:\n",
    "t2 = torch.zeros(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check how a tensor looks like you can easily print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As an array may contain different types of data (e.g: string, float, int, boolean), so can a tensor (although it is restricted to numerical types only).*\n",
    "\n",
    "Let's check what type of data is in t1 and t2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1.type(), t2.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type `LongTensor` is used to store integer values and `FloatTensor` for real values. You can also convert tensors from one type to another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_real = t1.float()\n",
    "print(t1, t1.type(), t1_real, t1_real.type())\n",
    "t2_int = t2.long()\n",
    "print(t2, t2.type(), t2_int, t2_int.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Another important property of a tensor is its shape, you can use the `.shape` property or `.size()` method to check the dimension of a tensor.* \n",
    "\n",
    "Both return a `torch.Size` object that can be manipulated with its own operations. Most of the time you will just use torch.Size objects as a list (e.g. to check the size of a tensor along one of its dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1.shape, t1.size())\n",
    "# if we want to check for the length of the tensor along its first dimension:\n",
    "print(t1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1D (vectors) and 2D (matrices) tensors**\n",
    "\n",
    "Let us now familiarize ourselves with some simple tensor operations. If you are familiar with the numpy library this will look alike, similarly if you know about matlab this should also remind you of what you can do with this language. If none of the above, you might be reconsidering your life choices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create 2 1D tensors of 5 random integers:\n",
    "v1 = torch.randint(low=-100, high=100, size=[5])\n",
    "v2 = torch.randint(low=-100, high=100, size=[5])\n",
    "v1, v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review some basic operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addition\n",
    "v_sum = v1 + v2\n",
    "print(v_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtraction (-), element-wise multiplication (*) and element-wise division follow the same logic.\n",
    "\n",
    "<span style=\"color:red; font-style:italic\">Your code comes below</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract v1 from v2\n",
    "v_sub = #your code\n",
    "print(v_sub)\n",
    "\n",
    "# Multiply the elements of v1 and v2\n",
    "v_mul = #your code\n",
    "print(v_mul)\n",
    "\n",
    "# Divide the elements of v1 by v2\n",
    "v_div = #your code\n",
    "print(v_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you would like to extract a subvector from the tensor, we call this operation *slicing* and it is as simple as this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing: extract sub-Tensor [from:to)\n",
    "print(v_sum[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also extract specific elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving first, fourth and fifth elements\n",
    "print(v_sum[[0, 3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within deep learning, the fun really starts when the dimensionality of the tensors increases. So let's have a look at matrices.\n",
    "Let's create two of them, again there are many ways of instanciating matrices -> Google is your friend whenever you look for [one of them](https://letmegooglethat.com/?q=Instantiate+matrix+in+pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first matrix will be a 5x 10 matrix full of normally distributed random values:\n",
    "m1 = torch.randn(5, 10)\n",
    "\n",
    "# The second is the 10x10 identity matrix:\n",
    "m2 = torch.eye(10)\n",
    "\n",
    "m1, m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slicing**\n",
    "\n",
    "Slicing in matrices (or any tensors) can be performed thanks to `:`, negative numbers count from the end. Feel free to play with this operator and to vizualize the results in order to be sure to understand what is slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1[:4, 2:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squeeze and Unsqueeze**\n",
    "\n",
    "Squeezing removes one dimension from the vector, on the opposite unsqueeze add a dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before unsqueeze: ', m1.shape)\n",
    "m1 = m1.unsqueeze(0)\n",
    "print('After unsqueeze: ', m1.shape)\n",
    "m1 = m1.squeeze(0)\n",
    "print('After squeeze: ', m1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expand**\n",
    "\n",
    "Expand will reproduce the tensor along the expanded dimensions, see the [docs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand).\n",
    "For example if we want to create a 4x6 matrix where each column is a vector of integer that goes from 1 to 6 we could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.arange(1, 7)\n",
    "m_v = v.unsqueeze(1).expand(-1, 4)\n",
    "m_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View**\n",
    "\n",
    "Sometimes a tensor has not the right shape, for example when you manipulated images you sometimes need to process them as vectors. The `.view` operator will create a view that has the new shape, by *view* we mean that nothing has changed in memory but the tensor can now be manipulated as it had this new shape.\n",
    "\n",
    "Let us imagine that we have 10 256x256 RGB images that we would like to manipulated as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(10, 3, 256, 256)\n",
    "images_as_vectors = images.view(10, -1)\n",
    "print(images.shape, images_as_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both variables model the same values though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.norm(images - images_as_vectors.view(10, 3, 256, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see below that if we change the values inside one of the view it will modify the tensor (which is common to both views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_as_vectors[0, :] = torch.zeros_like(images_as_vectors[0, :])\n",
    "print(torch.norm(images - images_as_vectors.view(10, 3, 256, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permute**\n",
    "\n",
    "The permute operation can be used to permuted between dimensions. For example you can transpose a matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.permute(1, 0), m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix multiplication**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@` operator can be used to do matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 @ m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other predefined primitives**\n",
    "\n",
    "Tensors in PyTorch already implements many useful methods such as `.mean()`, `.std()`, `.sum()` and many others. These functions may take many different arguments however you will often see the word `axis` in the docs. This word explicits on which dimension of the tensor the method must work. As an example we can create a $5 \\times 10$ matrix of normally distributed random values and compute the mean value, the mean of each row or of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_g = torch.randn(5, 10)\n",
    "m_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_g.mean(0) # Computes the mean value along the axis 0, it is to say the mean of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_g.mean(1) # Computes the mean value along the axis 1, it is to say the mean of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_g.mean() # Computes the global mean value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Spot and solve the bug\n",
    "\n",
    "Assume we would like to compute the mean of the following tensor, but we are not yet familiar enough with PyTorch and therefore encounter the following error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([8, 12, 80, 100])\n",
    "a.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-style:italic\">Fix it with one operation for us, your code comes below.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 You practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor with $10^{5}$ i.i.d. normally (mean 5 and standard deviation 2) distributed values.\n",
    "\n",
    "<span style=\"color:red; font-style:italic\">Your code comes below</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean value of the tensor with a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean and the standard deviation with the correct PyTorch operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe when you compare the running time of the two code snippets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green; font-style:italic\"> Explain in one sentence below</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get even more familiar with tensor manipulations trace the curve defined by this set of equations\n",
    "\n",
    "\n",
    "$t \\in [0, 2\\pi[$\n",
    "\n",
    "$x = 16 \\sin(t)^3$\n",
    "\n",
    "$y = 13 \\cos(t) - 5 \\cos(2t) - 2 \\cos(3t) - \\cos(4t)$\n",
    "\n",
    "For plotting purposes you can use the matplotlib library and the math library for the values of $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi as pi \n",
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can send the result to your favourite tinder date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The autograd package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograd package is what really makes PyTorch different from other algebraic language/libraries such as Matlab or Numpy. Whenever you make operations in PyTorch, it will create a computation graph that can later be used to compute derivatives of the output quantities with respect to the input or other intermediate computation steps. \n",
    "\n",
    "The official documentation says: *'torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s for which gradients should be computed with the requires_grad=True keyword. '*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 A guided tour\n",
    "\n",
    "Let's see how autograd may be used to compute the derivative of an analytical function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + 3*x - 2*torch.sin(x/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_df(x):\n",
    "    return 2*x + 3 - 2/5*torch.cos(x/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, requires_grad=True)\n",
    "y = f(x)\n",
    "df = torch.autograd.grad(y, x)\n",
    "\n",
    "x, y, df, hand_df(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see how autograd is able to compute the gradient of scalar values with respecto to a vector automatically for you. We can exploit this for example to perform least squares curve fitting with a simple parametric model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_function(x):\n",
    "    return (5*x + 3*x**2)*(x - 1)/2 + 3\n",
    "\n",
    "# generate some data\n",
    "x = torch.arange(-3, 3, .1)\n",
    "y = unknown_function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a parametric model\n",
    "def parametric_function(w, x):\n",
    "    x = x.unsqueeze(1)\n",
    "    return torch.cat((torch.ones_like(x), x, x**2, x**3), 1) @ w\n",
    "\n",
    "# random initalization of the parameters\n",
    "param = torch.randn(4, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what happens before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Before Training')\n",
    "plt.plot(x, y, label='Observed')\n",
    "plt.plot(x, parametric_function(param, x).detach(), label='Predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform gradient descent on the mean squared error loss function in order to fit the observed curve.\n",
    "Note how the torch.autograd.grad function gives us automatically access to the gradients which we then use during the optimization step. For more information about autograd you can check this short [tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) and the [documentation](https://pytorch.org/docs/stable/autograd.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .001\n",
    "for i in range(1000):\n",
    "    y_pred = parametric_function(param, x)\n",
    "    mse = ((y - y_pred)**2).mean()\n",
    "    grad = torch.autograd.grad(mse, param)[0]\n",
    "    param = param - lr * grad\n",
    "plt.figure()\n",
    "plt.title('After Training')\n",
    "plt.plot(x, y, label='Observed')\n",
    "plt.plot(x, parametric_function(param, x).detach(), label='Predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the gradient values into new variables can quickly become cumbersome, fortunately PyTorch developers found a solution to this, PyTorch **accumulates** the gradient values directly into the corresponding tensor (in the 'grad' property). To use this option you have to use the backward function on the quantity of interest and it will compute the gradient of this quantity with respect to each tensor that has the 'requires_grad' flag set to True. Later on in this notebook we will see how this, together with Optimizer objects, makes gradient descent very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10)\n",
    "w = torch.arange(10).float()\n",
    "w.requires_grad = True\n",
    "y = x.T @ w # Simple linear function\n",
    "y.backward()\n",
    "\n",
    "x, w, w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 You practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to perform the same kind of curve fitting that we did a few cells above, but this time we want to use the backward method instead of torch.autograd.grad(). Remember that PyTorch will keep accumulating the gradient values in the `.grad` property of each tensor. To deal with this you can reset the values of the gradients to 0 with the [`.zero_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.zero_) method. To make your life easier, you can start from the learning loop two cells above, be careful to reset the parameters before training and to not use the `torch.autograd.grad` function.\n",
    "\n",
    "<span style=\"color:red; font-style:italic\">Your code comes below</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The `torch.nn` and `torch.optim` packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to define your own neural network and train it with tensorial operations and the autograd package. However this would require you to explicitely define every operation in the neural network and to keep track of all the parameters for performing gradient descent. Fortunately, PyTorch provides the `torch.nn` and `torch.optim` packages which implement everything you need to define and train a neural network efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing both packages\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 A guided tour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nn.Functional`**\n",
    "\n",
    "The `nn` module implements many predefined functions which should simplify your life for building your own neural networks, the complete list can be accessed [here](https://pytorch.org/docs/stable/nn.functional.html). \n",
    "\n",
    "Let's play with some of them, below we see some commonly used functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-5, 5, .1)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "y1 = nn.functional.relu(x)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('ReLU')\n",
    "plt.plot(x, y1)\n",
    "\n",
    "y2 = nn.functional.leaky_relu(x, negative_slope=.2)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Leaky ReLU')\n",
    "plt.plot(x, y2)\n",
    "\n",
    "y3 = nn.functional.linear(x.unsqueeze(1), weight=torch.tensor([.5]))\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Linear')\n",
    "plt.plot(x, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nn.Module`**\n",
    "\n",
    "\n",
    "PyTorch provides a very important base class named `nn.Module`. This class is used to build complex neural networks. In fact any class which inherits from it will automatically keep track of the parameters of its components (or properties). To define your own `nn.Module` sub-class you only need to implement the `__init__` constructor and the forward function. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This means we create a class named \"MySimpleParametricModel\" which inherits from nn.Module.\n",
    "class MySimpleParametricModel(nn.Module): \n",
    "    # We could add arguments to the constructor however we do not need that here.\n",
    "    def __init__(self):\n",
    "        # We need to call the parent's constructor.\n",
    "        super(MySimpleParametricModel, self).__init__()\n",
    "        \n",
    "        # Here we add a property 'w' to the module which is itself a module that implements a linear layer.\n",
    "        self.w = nn.Linear(in_features=3, out_features=1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate an object of the `MySimpleParametricModel` class and have a look at its parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MySimpleParametricModel()\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that by inheritance from the `nn.Module` class, the parameters of any module used as a property of an object from the class `MySimpleParametricModel` is now accessible as a parameter of this object.\n",
    "The `nn.Module` implements other useful instructions such as user-friendly printing of the module which summarize the modules contained in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already implements many `nn.modules` that are often used as neural networks components such as convolutions, activations and linear layers.\n",
    "For example let's imagine we want to build a simple 3 layers MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleMLP(nn.Module):\n",
    "    def __init__(self, in_size, hidden_units, out_size):\n",
    "        super(MySimpleMLP, self).__init__()\n",
    "        \n",
    "        # Let us now define the linear layers we need:\n",
    "        self.fc1 = nn.Linear(in_size, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.fc3 = nn.Linear(hidden_units, out_size)\n",
    "        \n",
    "    # We have also to define what is the forward of this module:\n",
    "    def forward(self, x):\n",
    "        h1 = nn.functional.relu(self.fc1(x))\n",
    "        h2 = nn.functional.relu(self.fc2(h1))\n",
    "        out = self.fc3(h2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a 3 layers MLP (with 10 hidden neurons in each layer) that computes a scalar quantity from a scalar input.\n",
    "my_net = MySimpleMLP(1, 10, 1)\n",
    "\n",
    "# We usually give batches of values to nn modules, where the first dimension is the dimension of the batch while \n",
    "#the others must represent your data (here a scalar).\n",
    "x = torch.arange(-2, 2, .1).unsqueeze(1) \n",
    "\n",
    "# Detach is used to detach the tensor from its computation graph, it is required to \n",
    "#be able to convert the tensor as numpy matrix (which is implicitely made when you plot a tensor).\n",
    "y = my_net(x).detach() \n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nn.Sequential`**\n",
    "\n",
    "Writing the forward pass can become inconvenient and dirty if you want to add a multiple number of layers, for that you can use the `nn.Sequential` module which automatically chains modules with each others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyElegantSimpleMLP(nn.Module):\n",
    "    def __init__(self, in_size, hidden_units, out_size):\n",
    "        super(MyElegantSimpleMLP, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(nn.Linear(in_size, hidden_units), nn.ReLU(),\n",
    "                                nn.Linear(hidden_units, hidden_units), nn.ReLU(),\n",
    "                                nn.Linear(hidden_units, out_size))\n",
    "        \n",
    "    # We have also to define what is the forward of this module:\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a 3 layers MLP (with 30 hidden neurons in each layer) \n",
    "# that computes scalar quantity from scalar input.\n",
    "my_net = MyElegantSimpleMLP(1, 30, 1)\n",
    "\n",
    "x = torch.arange(-2, 2, .1).unsqueeze(1) \n",
    "y = my_net(x).detach() \n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer**\n",
    "\n",
    "You should now be able to define any kind of neural network you like. However if you want to optimize it you would still need to iterate through all the parameters of the net thanks to the `.parameters()` iterator. In principle you could do that to update the values accordingly to their gradient and to your update rule, however this would require some code and would be prone to bugs. Instead, the `nn.Optimizer` implements classes that handle that for you!\n",
    "For example let's say you would like to learn the function $y := f(x) = x^2$ with a neural network with mean squared error and stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an object from the class SGD that will make the updates for us.\n",
    "sgd_optimizer = optim.SGD(params=my_net.parameters(), lr=.001)\n",
    "\n",
    "# Let's do some learning steps with randomly generated x values:\n",
    "for i in range(5000):\n",
    "    x = torch.randn(100, 1)\n",
    "    y = x**2\n",
    "    y_pred = my_net(x)\n",
    "    \n",
    "    # We have to set all the grad values of the parameters of our net to zero, we can use zero_grad instruction\n",
    "    sgd_optimizer.zero_grad()\n",
    "    \n",
    "    # Let's compute the loss and the gradients with respect to it.\n",
    "    loss = ((y - y_pred)**2).mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    # And now we update the parameters:\n",
    "    sgd_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-2, 2, .1).unsqueeze(1) \n",
    "\n",
    "y = my_net(x).detach() \n",
    "plt.plot(x, y, label='Neural network')\n",
    "plt.plot(x, x**2, label='$x^2$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not perfect but we can see that the network learned to model something that looks like a quadratic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our first neural network classifier\n",
    "\n",
    "We now have all the necessary knowledge to build and train our very first neural network on a simple binary classification task. To do this we will start by importing a toy example dataset from the sklearn library, and then create and use the resulting splits for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X, Y = make_moons(500, noise=0.1) # create artificial data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=73) # create splits\n",
    "\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=Y_train) # visualize the data\n",
    "plt.title('Moon Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a neural network in PyTorch, no matter how complex the model is, we always go through a training loop. In this loop we feed the data to the model and get its predictions. We then compare the predictions of the network to the ground truth and adjust the parameters of the model by performing gradient descent. We have already seen all the components that are necessary for going through this process, so the only thing that remains to be done is to put all the pieces of this notebook together and train our **first awesome neural network!**\n",
    "\n",
    "During the training stage we would like to keep track whether our model will improve over the different iterations. It is therefore good practice to monitor whether the loss we are minimizing decreases over time, and whether the overall performance of the model increases the more training iterations we perform. Remember that one of the key components of Deep Learning are tensors, and we might not always have the data coming in this specific format. It is therefore necessary to convert it if needed.\n",
    "\n",
    "Once the data is in an appropriate format it can be given to the model, and we can obtain its predictions. This is what we usually call the **forward pass**. Once we obtain our predictions, we can compare how close they are to what we would like the network to predict: to do this we feed our predictions together with the true labels through the loss function which are minimizing. At its early training stages the network will very likely perform poorly: we can improve its performance by adjusting its weights by gradient descent. We can do this very easily by obtaining the gradients of the parameters with respect to the loss function we are minimizing (**backward pass**) and adjusting these weights with the optimizer we defined previously.\n",
    "\n",
    "Once all of this is done we can measure the performance of our model, which in this case will be reflected by its accuracy in classifying the synthetic dataset we created beforehand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(2, 50), nn.ReLU(),\n",
    "                   nn.Linear(50, 50), nn.ReLU(),\n",
    "                   nn.Linear(50, 1), nn.Sigmoid())\n",
    "optimizer = optim.Adam(net.parameters(), lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_hat, y):\n",
    "    return nn.BCELoss()(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = [] # where we keep track of the loss\n",
    "train_accuracy = [] # where we keep track of the accuracy of the model\n",
    "iters = 1000 # number of training iterations\n",
    "\n",
    "Y_train_t = torch.FloatTensor(Y_train).reshape(-1, 1) # re-arrange the data to an appropriate tensor\n",
    "\n",
    "for i in range(iters):\n",
    "    X_train_t = torch.FloatTensor(X_train)\n",
    "    y_hat = net(X_train_t) # forward pass\n",
    "    \n",
    "    loss = loss_func(y_hat, Y_train_t) # compute the loss\n",
    "    loss.backward() # obtain the gradients with respect to the loss\n",
    "    optimizer.step() # perform one step of gradient descent\n",
    "    optimizer.zero_grad() # reset the gradients to 0\n",
    "    \n",
    "    y_hat_class = np.where(y_hat.detach().numpy()<0.5, 0, 1) # we assign an appropriate label based on the network's prediction\n",
    "    accuracy = np.sum(Y_train.reshape(-1,1)==y_hat_class) / len(Y_train) # compute final accuracy\n",
    "    \n",
    "    train_accuracy.append(accuracy)\n",
    "    train_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Training Loss')\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (Binary Cross Entropy)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Training Accuracy')\n",
    "plt.plot(train_accuracy)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model, steps=1000, cmap='Paired'):\n",
    "    \"\"\"\n",
    "    Function to plot the decision boundary and data points of a model.\n",
    "    Data points are colored based on their actual label.\n",
    "    \"\"\"\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "\n",
    "    # Define region of interest by data limits\n",
    "    xmin, xmax = X[:,0].min() - .5, X[:,0].max() + .5\n",
    "    ymin, ymax = X[:,1].min() - .5, X[:,1].max() + .5\n",
    "    steps = 1000\n",
    "    x_span = np.linspace(xmin, xmax, steps)\n",
    "    y_span = np.linspace(ymin, ymax, steps)\n",
    "    xx, yy = np.meshgrid(x_span, y_span)\n",
    "\n",
    "    # Make predictions across region of interest\n",
    "    with torch.no_grad():\n",
    "        labels = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]))\n",
    "\n",
    "    # Plot decision boundary in region of interest\n",
    "    z = labels.reshape(xx.shape)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, z, cmap=cmap, alpha=0.5)\n",
    "\n",
    "    # Get predicted labels on training data and plot\n",
    "    train_labels = model(torch.FloatTensor(X))\n",
    "    ax.scatter(X[:,0], X[:,1], c=y.ravel(), cmap=cmap, lw=0)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "plot_decision_boundary(X_train,Y_train, net, cmap = 'RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Discuss the bug!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After actively following the theoretical lectures of professor Louppe, our favourite Italian student Al Dente remembered that a common loss function one can use when dealing with classification problems is the Cross-Entropy loss. However when implementing it in PyTorch Al remains disappointed since his code does not work anymore.\n",
    "\n",
    "What do you think is the cause of this bug? Maybe, first check the [documentation about `torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss).\n",
    "Briefly discuss your idea and **bonus** can you come up with a solution (this should be feasible with 3 modifications)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_dim = X_train.shape[1]\n",
    "n_hidden = 4 \n",
    "n_output = 1\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n_input_dim, n_hidden),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(n_hidden, n_output))\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss = [] \n",
    "train_accuracy = [] \n",
    "iters = 1000 \n",
    "\n",
    "Y_train_t = torch.FloatTensor(Y_train).reshape(-1, 1) \n",
    "for i in range(iters):\n",
    "    X_train_t = torch.FloatTensor(X_train)\n",
    "    y_hat = net(X_train_t) \n",
    "    loss = loss_func(y_hat, Y_train_t) \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    optimizer.zero_grad() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green; font-style:italic\"> Your explanation comes below</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 You practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to play around with the neural network yourself: you are free to experiment with whatever you think is best exploring. Your goal will be to come up with three modifications to the network we initially provided you with and to compare the performance of your modified models to the one we have given you beforehand. As potential modifications you could experiment with e.g. changing the depth or width of the model, modify the activation function, learning rate, optimizer, etc.\n",
    "\n",
    "Report your results with some appropriate plots and with a brief presentation of what you observe.\n",
    "\n",
    "<span style=\"color:red; font-style:italic\"> Your code comes below</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green; font-style:italic\"> Your discussion comes below </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "\n",
    "We will now ask you few questions to improve the content of this homework for next years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">How long did you spend on this homework?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Did you learn something?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Do you now feel comfortable with writing simple mathematical operations (like you would do in matlab or with numpy) in PyTorch?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Do you now feel comfortable using PyTorch for performing gradient descent?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Do you now feel comfortable with building and learning neural networks with PyTorch?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
